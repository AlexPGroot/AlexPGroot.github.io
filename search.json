[
  {
    "objectID": "004_C_elegans.html",
    "href": "004_C_elegans.html",
    "title": "C. elegans analysis using different compounds",
    "section": "",
    "text": "Library\nlibrary(here)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(drc)\nlibrary(gt)\ntheme_set(theme_bw())\n\n\n\n\nCode\ndata_0040 &lt;-\n  readxl::read_excel(here(\"data_raw/data_0040/CE.LIQ.FLOW.062_Tidydata.xlsx\"))\ndata_0040 %&gt;% head() %&gt;% gt::gt()\n\n\n\n\nTable 1: head Overview of the loaded data.\n\n\n\n\n\n\n\n\n\nplateRow\nplateColumn\nvialNr\ndropCode\nexpType\nexpReplicate\nexpName\nexpDate\nexpResearcher\nexpTime\nexpUnit\nexpVolumeCounted\nRawData\ncompCASRN\ncompName\ncompConcentration\ncompUnit\ncompDelivery\ncompVehicle\nelegansStrain\nelegansInput\nbacterialStrain\nbacterialTreatment\nbacterialOD600\nbacterialConcX\nbacterialVolume\nbacterialVolUnit\nincubationVial\nincubationVolume\nincubationUnit\nincubationMethod\nincubationRPM\nbubble\nincubateTemperature\n\n\n\n\nNA\nNA\n1\na\nexperiment\n3\nCE.LIQ.FLOW.062\n2020-11-30\nSergio Reijnders - Ellis Herder\n68\nhour\n50\n44\n24157-81-1\n2,6-diisopropylnaphthalene\n4.99\nnM\nLiquid\ncontrolVehicleA\nN2\n25\nOP50\nheated\n0.743\n8\n300\nul\n1,5 glass vial\n1000\nul\nrockroll\n35\nNA\n20\n\n\nNA\nNA\n1\nb\nexperiment\n3\nCE.LIQ.FLOW.062\n2020-11-30\nSergio Reijnders - Ellis Herder\n68\nhour\n50\n37\n24157-81-1\n2,6-diisopropylnaphthalene\n4.99\nnM\nLiquid\ncontrolVehicleA\nN2\n25\nOP50\nheated\n0.743\n8\n300\nul\n1,5 glass vial\n1000\nul\nrockroll\n35\nNA\n20\n\n\nNA\nNA\n1\nc\nexperiment\n3\nCE.LIQ.FLOW.062\n2020-11-30\nSergio Reijnders - Ellis Herder\n68\nhour\n50\n45\n24157-81-1\n2,6-diisopropylnaphthalene\n4.99\nnM\nLiquid\ncontrolVehicleA\nN2\n25\nOP50\nheated\n0.743\n8\n300\nul\n1,5 glass vial\n1000\nul\nrockroll\n35\nNA\n20\n\n\nNA\nNA\n1\nd\nexperiment\n3\nCE.LIQ.FLOW.062\n2020-11-30\nSergio Reijnders - Ellis Herder\n68\nhour\n50\n47\n24157-81-1\n2,6-diisopropylnaphthalene\n4.99\nnM\nLiquid\ncontrolVehicleA\nN2\n25\nOP50\nheated\n0.743\n8\n300\nul\n1,5 glass vial\n1000\nul\nrockroll\n35\nNA\n20\n\n\nNA\nNA\n1\ne\nexperiment\n3\nCE.LIQ.FLOW.062\n2020-11-30\nSergio Reijnders - Ellis Herder\n68\nhour\n50\n41\n24157-81-1\n2,6-diisopropylnaphthalene\n4.99\nnM\nLiquid\ncontrolVehicleA\nN2\n25\nOP50\nheated\n0.743\n8\n300\nul\n1,5 glass vial\n1000\nul\nrockroll\n35\nNA\n20\n\n\nNA\nNA\n2\na\nexperiment\n3\nCE.LIQ.FLOW.062\n2020-11-30\nSergio Reijnders - Ellis Herder\n68\nhour\n50\n35\n24157-81-1\n2,6-diisopropylnaphthalene\n4.99\nnM\nLiquid\ncontrolVehicleA\nN2\n25\nOP50\nheated\n0.743\n8\n300\nul\n1,5 glass vial\n1000\nul\nrockroll\n35\nNA\n20\n\n\n\n\n\n\n\n\n\n\nColumn RawData has missing data in cells 192-196, or samples in vialNr 3, compName napthalene. These data points are NA in the data when loaded into R studio. Most compUnits use nM, Ethanol and S-medium use pct.\n\n\nCode\ndata_0040_tidy &lt;- data_0040\ndata_0040_tidy$compConcentration &lt;- \n  as.numeric(data_0040$compConcentration)\n\n\nSince compConcentration is of character type, the data does not get plotted properly and we change it to numeric.\n\n\nCode\n# create ggplot, adding concentration to x-axis and measured data to y-axis\n# control/experiment are defined by shape, components are defined by colour\ndata_0040_graph &lt;- \n  as_tibble(data_0040_tidy) %&gt;%\n  ggplot(aes(x = compConcentration,\n             y = RawData,\n             shape = expType,\n             colour = compName)) +\n  geom_point(position = position_jitter(w = 0.03, h = 0)) + \n  scale_x_continuous(trans = log10_trans()) + # log10 scale is applied to make results more readable.\n  labs(title = \"Effect of compounds on C. elegans offspring\") +\n  ylab(\"Offspring Count\") +\n  xlab(\"Compound Concentration\")\nprint(data_0040_graph)\n\n\n\n\n\n\n\n\nFigure 1: Scatter plot showing the number of C. elegans offspring in response to incubation of adult nematods to varying concentrations of different compounds. Compounds are measured in nanomolar (nM), except for ethanol and S-medium (%).\n\n\n\n\n\n\nFigure 1 shows a scatter plot of the C. elegans offspring data after the adults have been incubated in differing concentrations of the different compounds; 2,6-diisopropylnapthalene, decane, napthalene, the positive control ethanol and the negative control S-medium. As the differences in concentrations were quite large, we changed to a log10 scale to more clearly present the data. We also added a slight jitter to the data points to displace each point and make it easier to read, as a lack of jitter results in a straight vertical line of data points.\n\n\nCode\n# facet wrap to show individual graphs\ndata_0040_graph +\n  facet_wrap(vars(compName))\n\n\n\n\n\n\n\n\nFigure 2: Scatter plot showing the number of C. elegans offspring in response to incubation of adult nematods to varying concentrations of different compounds. Compounds are measured in nanomolar (nM), except for ethanol and S-medium (%). The plots are now split into individual plots to better show each trend.\n\n\n\n\n\n With the each component split into its own plot in Figure 2 using facet_wrap it is easier to see the downward trend in the offspring count once the concentration of the components increases. \n\n\nCode\n# data is normalized for negative control by setting the mean of negative control to 1\n\nnegative_control_mean &lt;- data_0040_tidy %&gt;%\n  filter(expType == \"controlNegative\") %&gt;%\n  summarise(mean_value = mean(RawData, na.rm = TRUE)) %&gt;%\n  pull(mean_value) # get the value from the mean_value in the tibble without having to use $mean_value\n\n# normalize the data:\n# if the data is negative control, set value to 1\n# else divide raw data by negative control mean to create value as a fraction\ndata_0040_temp &lt;- data_0040_tidy\ndata_0040_normalized &lt;- data_0040_temp %&gt;%\n  mutate(\n    NormalizedData = if_else(expType == \"controlNegative\", 1, RawData / negative_control_mean)\n  )\n\n# print normalized dataset as ggplot\ndata_0040_normalized_graph &lt;-\n  data_0040_normalized %&gt;%\n  ggplot(aes(x = compConcentration,\n             y = NormalizedData,\n             shape = expType,\n             colour = compName)) +\n  geom_point(position = position_jitter(w = 0.03, h = 0)) +\n  scale_x_log10() +\n  labs(title = \"Effect of compounds on C. elegans offspring\",\n       subtitle = \"Normalized data compared to negative control\") +\n  xlab(\"Compound concentration\") +\n  ylab(\"Normalized offspring count\")\nprint(data_0040_normalized_graph)\n\n\n\n\n\n\n\n\nFigure 3: Scatter plot showing the number of C. elegans offspring in response to incubation of adult nematods to varying concentrations of different compounds. Compounds are measured in nanomolar (nM), except for ethanol and S-medium (%). The data is normalized and shows the effect of the compound on the offspring count compared to the negative control at the 1.0 baseline.\n\n\n\n\n\nAs the nematodes grow differently due to variance we have different amounts of offspring per adult. In Figure 3 the data has been normalized using the negative control as a baseline to show the impact of the compounds on the offspring count.\n\nCode\ndata_0040_filter_exp &lt;- data_0040_tidy %&gt;% \n  filter(expType == \"experiment\")\n\n# data_0040_filter_exp_naphthalene &lt;-\n#   data_0040_filter_exp %&gt;% \n#   filter(compName == \"naphthalene\")\n# \n# drm_nap &lt;- \n#   drm(data = data_0040_filter_exp_naphthalene,\n#     formula = RawData ~ compConcentration,\n#     fct = LL.4())\n# \n# plot(drm_nap, main = \"naphthalene\")\n\n####\n\nexp_compname &lt;- data_0040_filter_exp$compName %&gt;% unique()\n\nfor (i in exp_compname) {\n  drm_calc &lt;- drm(data = data_0040_normalized %&gt;% filter(compName == i),\n               formula = NormalizedData ~ compConcentration,\n               fct = LL.4())\n  \n  plot(drm_calc, \n       main = i, \n       ylab = \"Normalized Offspring Count\",\n       xlab = \"Compound Concentration (nM)\")\n\n  # calc EC50 using ED, extract [1] ec50 value\n  ec50 &lt;- round(ED(drm_calc, 50)[1], digits = 2)\n  \n  # Add EC50 annotation to the plot\n  abline(h = 0.5, lty = 2) # line hor at 0.5 offsprint count, line ver at ec50, lty dashed lines\n  abline(v = ec50, lty = 2) \n  text(x = ec50, \n       y = 0.6,\n       labels = paste(\"EC50: \", ec50, \"nM\"),\n       pos = 2) # position left of point\n}\n\n\n\n\n\n\n\nEstimated effective doses\n\n       Estimate Std. Error\ne:1:50 0.025072   0.010998\n\n\n\n\n\nEstimated effective doses\n\n       Estimate Std. Error\ne:1:50 0.119453   0.042934\n\n\n\n\n\nEstimated effective doses\n\n       Estimate Std. Error\ne:1:50    3.129        NaN\n\n\n\n\n\n\n\n\n\n\n(a) 2,6-diisopropylnaphthalene DRC showing a gradual decrease response by increasing the compound conentration. With a calculated EC50 of 0.03 (nM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) decane DRC showing a steep decreased response after the concentration of 0.0499 (nM). With a calculated EC50 of 0.12 (nM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) naphthalene DRC showing a steep decreased response around the concentration of 1.95 (nM). With a calculated EC50 of 3.13 (nM)\n\n\n\n\n\n\n\nFigure 4: Dose response curves (DRC) of each experimental compound using offspring count data that has been normalized for the negative control and their calculated EC50s\n\n\n\nDose response curves (DRCs) have been plotted using the drc package using the normalized offspring data and the compound concentrations. Using de ED function we also calculate the EC50 values and add them to the plots. In Figure 4 (a) we see 2,6-diisopropylnaphthalene has an EC50 of 0.03 (nM) and the normalized offsrping count gradually goes down when the compound concentration goes up. The EC50 however doesn’t intersect with the graph properly as it should be more around the range of 0.49 (nM) following the line. In Figure 4 (b) We see decane have an EC50 of 0.12 (nM). The graph shows a steep decrease in normalized offspring count after 0.0499 (nM). The calculated EC50 doesn’t quite intersect at the 50% as the compound concentration seems to be higher on the graph with 0.12 (nM) reaching about 60%. In Figure 4 (c) we see naphthalene with an EC50 of 3.13 (nM). The offspring count has a steep decrease after about 1.95 (nM) of compound.",
    "crumbs": [
      "Home",
      "C elegans analysis"
    ]
  },
  {
    "objectID": "001_CV.html",
    "href": "001_CV.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nCurriculum Vitae\nAbout me:\nStudied life sciences with major in molecular biology, transitioning into the field of data science. Having experience in R programming and bash, I am eager to use my analytical skills on complex data sets combined with programming to gain data-driven insights into biological processes. Or to build on projects that help facilitate and advance the process of analytics and research.\nPersonalia Name   Alex Groot  Location   Haarlemmermeer, The Netherlands  Email   Alex_groot@icloud.com \nEducation  Biologie en Medisch Laboratoriumonderzoek HBO Bachelor  Specialisatie Biomolecular Research \nData science skills  R (programming language) R Quarto  R Shiny  Git Data visualisation Bash Microsoft Excel\nWorkexperience Project Nanopore:  Processing MinION Nanopore RNA-seq data IGV browser\nProject genes and proteins: Transfecting eukaryote cells with recombinant plasmids SDS-PAGE Western-blot\nProject sphingolipids: Literature research Working with multiple cell ines (SKNAS, HEK293T, HEPG2). Setting up and performing assays (MTT, Amplex Red) Thin Layer Chromatography (TLC)",
    "crumbs": [
      "Home",
      "Curriculum Vitae"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This portfolio website is created using Quarto."
  },
  {
    "objectID": "vitae/vitae.html",
    "href": "vitae/vitae.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nAbout Me\nStudent HBO life sciences at Hogeschool Utrecht majoring in molecular biology, transitioning into the field of data science. Having experience in R programming and data science, I am eager to use my analytical skills and programming on biological data to gain data-driven insights into processes. I also enjoy working on projects that assist in facilitating the process of analytics and research.\n\n\nSkills\n\nR (programming language)\nR Quarto\nR Shiny\nData visualization (ggplot2/plotly)\nGit\nBash\nLinux\nMicrosoft Excel\n\n\n\nProjects\n\n\n# A tibble: 4 × 5\n  what                                                   when  with  where why  \n  &lt;chr&gt;                                                  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt;\n1 Processing and analyzing MinION RNA-sequencing data    \"\"    Proj… \"\"    &lt;chr&gt;\n2 Personal project using breast cancer dataset           \"\"    Gett… \"\"    &lt;chr&gt;\n3 Transfecting eukaryote cells with recombinant plasmids \"\"    Proj… \"\"    &lt;chr&gt;\n4 Cell culture and experimentation                       \"\"    Proj… \"\"    &lt;chr&gt;"
  },
  {
    "objectID": "003_Guerrilla_analytics_framework.html",
    "href": "003_Guerrilla_analytics_framework.html",
    "title": "Guerrilla analytics",
    "section": "",
    "text": "Guerrilla Analytics is data analytics performed in a very dynamic project environment that presents the team with varied and frequent disruptions and constrains the team in terms of the resources they can bring to bear on their analytics problem.  From: Guerrilla Analytics, 2015\nData science projects can get big, and keeping all files and folders tidy makes sure that it will stay possible to go through all the files and see how the analyses are performed. Keeping a well managed structure also allows for easier reproduction of the performed analyses when the folders are sent to other data scientist.  First we take a look at what a general folder structure might look like after a few weeks of assignments:\n\n\n\n\n\n\nOld folder structure:\n\n\nfs::dir_tree(path = “/Users/Alex/Documents/daur2/daurII_students_OLD”)  /Users/Alex/Documents/daur2/daurII_students_OLD  ├── Alex_Groot copy.csv  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.Rmd  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.html  ├── Andy.csv  ├── David.csv  ├── EXAM  │ ├── capstone_exam.Rmd  │ ├── capstone_exam.html  │ └── capstone_exam.md  ├── GDS858.soft.gz  ├── GPL11154.soft  ├── GPL96.soft  ├── GSE116936  │ └── GSE116936_RAW.tar  ├── GSE116936_series_matrix.txt.gz  ├── GSE116936_supp_files  │ ├── GSM3264576_A1-Ctrl.txt.gz  │ ├── GSM3264577_A2-Ctrl.txt.gz  │ ├── GSM3264578_A3-Ctrl.txt.gz  │ ├── GSM3264579_B1-PGE2.txt.gz  │ ├── GSM3264580_B2-PGE2.txt.gz  │ ├── GSM3264581_B3-PGE2.txt.gz  │ ├── GSM3264582_I1-TNF.txt.gz  │ ├── GSM3264583_I2-TNF.txt.gz  │ ├── GSM3264584_I3-TNF.txt.gz  │ ├── GSM3264585_J1-TNF+PGE2.txt.gz  │ ├── GSM3264586_J2-TNF+PGE2.txt.gz  │ └── GSM3264587_J3-TNF+PGE2.txt.gz  ├── GSE148829  │ ├── GSE148829_BEAS_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_Human1_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_Human2_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_Human_Ileum_absorptiveAndCryptentero_dge.csv.gz  │ ├── GSE148829_Human_lung_epithelial_cell_raw_counts.txt.gz  │ ├── GSE148829_Mouse_Basal_Pops_TPM.txt.gz  │ ├── GSE148829_NHP_TB_Grans_and_Lung_dge.txt.gz  │ ├── GSE148829_NHP_ileum_epithelial_raw_expression_counts.csv.gz  │ ├── GSE148829_NHP_lung_epithelial_raw_expression_counts.csv.gz  │ ├── GSE148829_NasalSSS_cleaned_raw_expression_counts.csv.gz  │ ├── GSE148829_RAW  │ │ ├── GSM4487938_Mouse_Nasal_IFNa_NH11_dge.txt.gz  │ │ ├── GSM4487939_Mouse_Nasal_IFNa_NH12_dge.txt.gz  │ │ ├── GSM4487940_Mouse_Nasal_IFNa_NH21_dge.txt.gz  │ │ ├── GSM4487941_Mouse_Nasal_IFNa_NH22_dge.txt.gz  │ │ ├── GSM4487942_Mouse_Nasal_IFNa_NS11_dge.txt.gz  │ │ ├── GSM4487943_Mouse_Nasal_IFNa_NS12_dge.txt.gz  │ │ ├── GSM4487944_Mouse_Nasal_IFNa_NS21_dge.txt.gz  │ │ ├── GSM4487945_Mouse_Nasal_IFNa_NS22_dge.txt.gz  │ │ ├── GSM4488219_Mouse_basal_stim_IFNA_0.1_REP1_TPM.txt.gz  │ │ ├── GSM4488220_Mouse_basal_stim_IFNA_0.1_REP2_TPM.txt.gz  │ │ ├── GSM4488221_Mouse_basal_stim_IFNA_0.1_REP3_TPM.txt.gz  │ │ ├── GSM4488222_Mouse_basal_stim_IFNA_0.5_REP1_TPM.txt.gz  │ │ ├── GSM4488223_Mouse_basal_stim_IFNA_0.5_REP2_TPM.txt.gz  │ │ ├── GSM4488224_Mouse_basal_stim_IFNA_0.5_REP3_TPM.txt.gz  │ │ ├── GSM4488225_Mouse_basal_stim_IFNB_0.1_REP1_TPM.txt.gz  │ │ ├── GSM4488226_Mouse_basal_stim_IFNB_0.1_REP2_TPM.txt.gz  │ │ ├── GSM4488227_Mouse_basal_stim_IFNB_0.1_REP3_TPM.txt.gz  │ │ ├── GSM4488228_Mouse_basal_stim_IFNB_0.5_REP1_TPM.txt.gz  │ │ ├── GSM4488229_Mouse_basal_stim_IFNB_0.5_REP2_TPM.txt.gz  │ │ ├── GSM4488230_Mouse_basal_stim_IFNB_0.5_REP3_TPM.txt.gz  │ │ ├── GSM4488231_Mouse_basal_stim_IFNA_1_REP1_TPM.txt.gz  │ │ ├── GSM4488232_Mouse_basal_stim_IFNA_1_REP2_TPM.txt.gz  │ │ ├── GSM4488233_Mouse_basal_stim_IFNA_1_REP3_TPM.txt.gz  │ │ ├── GSM4488234_Mouse_basal_stim_IFNA_2_REP1_TPM.txt.gz  │ │ ├── GSM4488235_Mouse_basal_stim_IFNA_2_REP2_TPM.txt.gz  │ │ ├── GSM4488236_Mouse_basal_stim_IFNA_2_REP3_TPM.txt.gz  │ │ ├── GSM4488237_Mouse_basal_stim_IFNB_1_REP1_TPM.txt.gz  │ │ ├── GSM4488238_Mouse_basal_stim_IFNB_1_REP2_TPM.txt.gz  │ │ ├── GSM4488239_Mouse_basal_stim_IFNB_1_REP3_TPM.txt.gz  │ │ ├── GSM4488240_Mouse_basal_stim_IFNB_2_REP1_TPM.txt.gz  │ │ ├── GSM4488241_Mouse_basal_stim_IFNB_2_REP2_TPM.txt.gz  │ │ ├── GSM4488242_Mouse_basal_stim_IFNB_2_REP3_TPM.txt.gz  │ │ ├── GSM4488243_Mouse_basal_stim_IFNA_5_REP1_TPM.txt.gz  │ │ ├── GSM4488244_Mouse_basal_stim_IFNA_5_REP2_TPM.txt.gz  │ │ ├── GSM4488245_Mouse_basal_stim_IFNA_5_REP3_TPM.txt.gz  │ │ ├── GSM4488246_Mouse_basal_stim_IFNA_10_REP1_TPM.txt.gz  │ │ ├── GSM4488247_Mouse_basal_stim_IFNA_10_REP2_TPM.txt.gz  │ │ ├── GSM4488248_Mouse_basal_stim_IFNA_10_REP3_TPM.txt.gz  │ │ ├── GSM4488249_Mouse_basal_stim_IFNB_5_REP1_TPM.txt.gz  │ │ ├── GSM4488250_Mouse_basal_stim_IFNB_5_REP2_TPM.txt.gz  │ │ ├── GSM4488251_Mouse_basal_stim_IFNB_5_REP3_TPM.txt.gz  │ │ ├── GSM4488252_Mouse_basal_stim_IFNB_10_REP1_TPM.txt.gz  │ │ ├── GSM4488253_Mouse_basal_stim_IFNB_10_REP2_TPM.txt.gz  │ │ ├── GSM4488254_Mouse_basal_stim_IFNB_10_REP3_TPM.txt.gz  │ │ ├── GSM4488255_Mouse_basal_stim_IFNG_0.1_REP1_TPM.txt.gz  │ │ ├── GSM4488256_Mouse_basal_stim_IFNG_0.1_REP2_TPM.txt.gz  │ │ ├── GSM4488257_Mouse_basal_stim_IFNG_0.1_REP3_TPM.txt.gz  │ │ ├── GSM4488258_Mouse_basal_stim_IFNG_0.5_REP1_TPM.txt.gz  │ │ ├── GSM4488259_Mouse_basal_stim_IFNG_0.5_REP2_TPM.txt.gz  │ │ ├── GSM4488260_Mouse_basal_stim_IFNG_0.5_REP3_TPM.txt.gz  │ │ ├── GSM4488261_Mouse_basal_stim_IFNG_1_REP1_TPM.txt.gz  │ │ ├── GSM4488262_Mouse_basal_stim_IFNG_1_REP2_TPM.txt.gz  │ │ ├── GSM4488263_Mouse_basal_stim_IFNG_1_REP3_TPM.txt.gz  │ │ ├── GSM4488264_Mouse_basal_stim_IFNG_2_REP1_TPM.txt.gz  │ │ ├── GSM4488265_Mouse_basal_stim_IFNG_2_REP2_TPM.txt.gz  │ │ ├── GSM4488266_Mouse_basal_stim_IFNG_2_REP3_TPM.txt.gz  │ │ ├── GSM4488267_Mouse_basal_stim_IFNG_5_REP1_TPM.txt.gz  │ │ ├── GSM4488268_Mouse_basal_stim_IFNG_5_REP2_TPM.txt.gz  │ │ ├── GSM4488269_Mouse_basal_stim_IFNG_5_REP3_TPM.txt.gz  │ │ ├── GSM4488270_Mouse_basal_stim_IFNG_10_REP1_TPM.txt.gz  │ │ ├── GSM4488271_Mouse_basal_stim_IFNG_10_REP2_TPM.txt.gz  │ │ ├── GSM4488272_Mouse_basal_stim_IFNG_10_REP3_TPM.txt.gz  │ │ ├── GSM4488273_Mouse_basal_stim_Untreated_0_REP1_TPM.txt.gz  │ │ ├── GSM4488274_Mouse_basal_stim_Untreated_0_REP2_TPM.txt.gz  │ │ ├── GSM4488275_Mouse_basal_stim_Untreated_0_REP3_TPM.txt.gz  │ │ ├── GSM4488276_Mouse_basal_stim_Untreated_0_REP4_TPM.txt.gz  │ │ ├── GSM4488277_Mouse_basal_stim_Untreated_0_REP5_TPM.txt.gz  │ │ ├── GSM4488278_Mouse_basal_stim_Untreated_0_REP6_TPM.txt.gz  │ │ ├── GSM4488279_Mouse_basal_stim_Untreated_0_REP7_TPM.txt.gz  │ │ ├── GSM4488280_Mouse_basal_stim_Untreated_0_REP8_TPM.txt.gz  │ │ ├── GSM4488281_Mouse_basal_stim_Untreated_0_REP9_TPM.txt.gz  │ │ ├── GSM4488282_Mouse_basal_stim_Untreated_0_REP10_TPM.txt.gz  │ │ ├── GSM4488283_Mouse_basal_stim_Untreated_0_REP11_TPM.txt.gz  │ │ └── GSM4488284_Mouse_basal_stim_Untreated_0_REP12_TPM.txt.gz  │ └── GSE148829_RAW.tar  ├── GSE75073  │ ├── GSE75073_RAW.tar  │ ├── GSE75073_Signals_450k.txt.gz  │ └── GSE75073_Signals_Epic.txt.gz  ├── GSE76073  │ ├── GSE76073_Normalized_counts.txt.gz  │ └── GSE76073_Raw_counts.txt.gz  ├── GSE923_series_matrix.txt.gz  ├── Images  │ ├── 2020-04-20CORT.png  │ ├── 20200403_citrul.png  │ ├── 20200404_citrul.png  │ ├── 20200404_ifabp.png  │ ├── Bioconductor.png  │ ├── Biostrings.png  │ ├── BrowseSeq  │ │ ├── Dia1.PNG  │ │ └── sequence_alignment_plots  │ │ ├── Dia1.PNG  │ │ ├── Dia2.PNG  │ │ ├── Dia3.PNG  │ │ ├── Dia4.PNG  │ │ ├── Dia5.PNG  │ │ ├── Dia6.PNG  │ │ ├── Dia7.PNG  │ │ └── Dia8.PNG  │ ├── DESeq2.png  │ ├── Dia1.JPG  │ ├── Dia3.JPG  │ ├── GenomicRanges.png  │ ├── Git_Setup_1.JPG  │ ├── Glimma.png  │ ├── IRanges.png  │ ├── MSnbase.png  │ ├── Project_1.jpg  │ ├── Project_2.jpg  │ ├── Rlogo.png  │ ├── Rtools.png  │ ├── bioconductorlogo.jpg  │ ├── cat_sleeping_on_keyboard.jpg  │ ├── citrulline.png  │ ├── connectome-1.png  │ ├── cortline.png  │ ├── covid19_ziekenhuis.png  │ ├── dotdotdot_rotate_axis.png  │ ├── edgeR.png  │ ├── exercise_2_1_1.png  │ ├── exercise_2_1_1_c.png  │ ├── forcats.png  │ ├── git_web.png  │ ├── gitinstallermac.png  │ ├── heatmap.png  │ ├── heatmap1.png  │ ├── heatmap2.png  │ ├── hex-ggplot2.png  │ ├── installpackages.png  │ ├── loadpackages.png  │ ├── logo_R.jpg  │ ├── ma1.png  │ ├── ma2.png  │ ├── mzR_hl.png  │ ├── new_proj_done.png  │ ├── new_proj_git.png  │ ├── one-doesnot-simply.jpg  │ ├── open_486.jpg  │ ├── open_compaq.jpg  │ ├── pca1.png  │ ├── pepper.jpg  │ ├── pipe.png  │ ├── purrr_small.png  │ ├── purrr_sticker.jpg  │ ├── pvalues.png  │ ├── r4ds.png  │ ├── readr_sticker.png  │ ├── readxl_sticker.png  │ ├── rmac_download.png  │ ├── rstudio_download.png  │ ├── rstudio_leeg.png  │ ├── rwin_download.png  │ ├── security.png  │ ├── setup-wizard-rstudio.png  │ ├── setupRtools.png  │ ├── stringr.png  │ ├── summarizedexperiment.png  │ ├── tidyr_sticker.png  │ ├── tidyverse_sticker.png  │ ├── usrlocal.png  │ ├── wizardRmac.png  │ ├── wizardRwindows.png  │ ├── wontinstall.png  │ ├── workflow_course.png  │ ├── xquarkdisk.png  │ ├── xquarkwizard.png  │ ├── xquartz.png  │ └── yoda_the_last_jedi.jpg  ├── John.csv  ├── Les 6.Rmd  ├── Les4.Rmd  ├── Les4.nb.html  ├── Les5.Rmd  ├── Lesson_5_Exercise_a1.Rmd  ├── Mike.csv  ├── R  │ └── do_tidy_pertussis.R  ├── README.md  ├── Steve.csv  ├── answers  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.Rmd  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.html  │ ├── daur2_lesson51_demo_multiple_sequence_alignment_files  │ │ └── figure-html  │ │ ├── unnamed-chunk-23-1.png  │ │ ├── unnamed-chunk-24-1.png  │ │ ├── unnamed-chunk-29-1.png  │ │ └── unnamed-chunk-30-1.png  │ ├── daur2_lesson_6_exercise_rnaseq.Rmd  │ ├── daur2_lesson_6_exercise_rnaseq.html  │ ├── daur2_lesson_6_exercise_rnaseq_student_version.Rmd  │ └── daur2_lesson_6_exercise_rnaseq_student_version.html  ├── caspase_align.aux  ├── caspase_align.log  ├── caspase_align.tex  ├── code  │ └── load_data.R  ├── data  │ ├── CISID_pertussis_10082018.csv  │ ├── biomaRt_homology_example.rds  │ ├── biomaRt_homology_example_aa.rds  │ ├── biomaRt_homology_example_dna.rds  │ ├── chircus_blg_aminoacids.fasta  │ ├── course_r_packages.tsv  │ ├── course_r_packages.txt  │ ├── covid_rivm  │ ├── go_bp-hit.csv  │ ├── go_cc-hit.csv  │ ├── go_mf-hit.csv  │ ├── hobbit_chapter.txt  │ ├── hobbit_dwarves.txt  │ ├── hox_aa_sequences.txt  │ ├── hox_dna_sequences.txt  │ ├── human_hoxb1_protein.fasta  │ ├── human_hoxb1_protein.gb  │ ├── human_rhodopsin_protein.fasta  │ ├── human_ttn_genomic_dna.fasta  │ ├── jp_coldata.txt  │ ├── jp_counts.txt  │ ├── jp_features.txt  │ ├── jp_miame.rds  │ ├── jp_miame.txt  │ ├── lesson2  │ │ ├── ENST00000642929.1.fa  │ │ ├── ENST00000643997.1.fa  │ │ ├── ENST00000644264.1.fa  │ │ ├── ENST00000644696.1.fa  │ │ ├── ENST00000644919.1.fa  │ │ ├── ENST00000646017.1.fa  │ │ ├── ENST00000646618.2.fa  │ │ ├── ENST00000646644.1.fa  │ │ ├── ENST00000647220.1.fa  │ │ ├── ENST00000647443.1.fa  │ │ ├── cholesterol.xlsx  │ │ ├── diet  │ │ │ ├── Andy.csv  │ │ │ ├── David.csv  │ │ │ ├── John.csv  │ │ │ ├── Mike.csv  │ │ │ └── Steve.csv  │ │ ├── diet_data.zip  │ │ └── sequence1.txt  │ └── lesson3  │ ├── GO-terms.txt  │ ├── dna_random  │ ├── sequence.fasta  │ ├── sequence1.txt  │ └── sequence2.txt  ├── daurII_students  │ ├── R  │ │ └── do_tidy_pertussis.R  │ ├── README.md  │ ├── code  │ │ └── load_data.R  │ ├── data  │ │ ├── CISID_pertussis_10082018.csv  │ │ ├── course_r_packages.tsv  │ │ ├── covid_rivm  │ │ ├── go_bp-hit.csv  │ │ ├── go_cc-hit.csv  │ │ ├── go_mf-hit.csv  │ │ ├── hobbit_chapter.txt  │ │ ├── hobbit_dwarves.txt  │ │ ├── hox_aa_sequences.txt  │ │ ├── hox_dna_sequences.txt  │ │ ├── human_hoxb1_protein.fasta  │ │ ├── human_hoxb1_protein.gb  │ │ ├── human_rhodopsin_protein.fasta  │ │ ├── human_ttn_genomic_dna.fasta  │ │ └── lesson2  │ │ └── diet_data.zip  │ ├── daurII_students.Rproj  │ └── demos  │ └── 01_demo_functions.Rmd  ├── daurII_students.Rproj  ├── demos  │ ├── 01_demo_functions.Rmd  │ ├── 021_demo_2_agenda.Rmd  │ ├── 021_demo_2_agenda.html  │ ├── 022_lesson2_map_nest.Rmd  │ ├── 022_lesson2_map_nest.html  │ ├── 031_strings_and_regex.Rmd  │ ├── 031_strings_and_regex.html  │ ├── 031_strings_and_regex.pdf  │ ├── 041_biomart.Rmd  │ ├── 042_geoquery.Rmd  │ ├── 043_rentrez.Rmd  │ ├── 051_demo_multiple_sequence_alignment.Rmd  │ ├── 051_demo_multiple_sequence_alignment.html  │ ├── 051_demo_multiple_sequence_alignment.md  │ ├── 051_demo_multiple_sequence_alignment_files  │ │ └── figure-html  │ │ ├── unnamed-chunk-23-1.png  │ │ ├── unnamed-chunk-24-1.png  │ │ ├── unnamed-chunk-29-1.png  │ │ └── unnamed-chunk-30-1.png  │ ├── 052_demo_summarized_experiment.Rmd  │ ├── 052_demo_summarized_experiment.html  │ ├── 052_demo_summarized_experiment.md  │ ├── 61_heatmaps.Rmd  │ ├── 61_heatmaps.html  │ ├── 62_rnseq_workflow.Rmd  │ └── 62_rnseq_workflow.html  ├── les1.Rmd  ├── les1.nb.html  ├── les2.Rmd  ├── les2.nb.html  ├── les3.Rmd  ├── my_transcript.fasta  ├── resources.Rmd  └── unzip  ├── Andy.csv  ├── David.csv  ├── John.csv  ├── Mike.csv  └── Steve.csv \n\n\n\n  As you can see, the original file structure used during the earlier data science course is quite the mess. Instead we now make usage of the Guerilla analytics framework to store data and manage files. Using simple structures to store data, code, generated graphs and makes it easier to use and replicate analyses, even years after the analyses are performed.  Making sure all files in a environment pertain only to that single project is important. And to make data storage simple, a central data folder in the root of the enviornment is used with clear naming conventions, dates and versions. Also metadata files are used to allow anyone with access to the project to see what the contents of the project are.  Here is the above file structure, with the guerilla analytics framework retroactively applied: \n\n\n\nNew folder structure:\n\n\n├── EXAM  │ ├── Alex_Groot copy.csv  │ ├── capstone_exam.Rmd  │ ├── capstone_exam.html  │ └── capstone_exam.md  ├── Images  │ ├── 2020-04-20CORT.png  │ ├── 20200403_citrul.png  │ ├── BrowseSeq  │ │ ├── Dia1.PNG  │ │ └── sequence_alignment_plots  │ │ ├── Dia1.PNG  │ │ ├── Dia2.PNG  │ └── yoda_the_last_jedi.jpg  ├── R  │ └── do_tidy_pertussis.R  ├── README.md  ├── caspase_align.aux  ├── caspase_align.log  ├── caspase_align.tex  ├── code  │ └── load_data.R  ├── data_raw  │ ├── lesson2  │ │ ├── ENST00000642929.1.fa  │ │ ├── ENST00000643997.1.fa  │ │ ├── ENST00000644264.1.fa  │ │ ├── cholesterol.xlsx  │ │ ├── diet  │ │ │ ├── Andy.csv  │ │ │ └── Steve.csv  │ │ ├── diet_data.zip  │ │ └── sequence1.txt  │ ├── lesson3  │ │ ├── GO-terms.txt  │ │ ├── dna_random  │ │ ├── sequence.fasta  │ │ ├── sequence1.txt  │ │ └── sequence2.txt  │ └── unsorted  │ ├── Andy.csv  │ ├── CISID_pertussis_10082018.csv  │ ├── David.csv  │ ├── GSE116936  │ │ └── GSE116936_RAW.tar  │ ├── GSE116936_series_matrix.txt.gz  │ ├── GSE116936_supp_files  │ │ ├── GSM3264576_A1-Ctrl.txt.gz  │ │ ├── GSM3264577_A2-Ctrl.txt.gz  │ │ ├── GSM3264578_A3-Ctrl.txt.gz  │ │ ├── GSM3264585_J1-TNF+PGE2.txt.gz  │ │ ├── GSM3264586_J2-TNF+PGE2.txt.gz  │ │ └── GSM3264587_J3-TNF+PGE2.txt.gz  │ ├── GSE148829  │ │ ├── GSE148829_BEAS_Basal_Pops_TPM.txt.gz  │ │ ├── GSE148829_Human1_Basal_Pops_TPM.txt.gz  │ │ ├── GSE148829_Human2_Basal_Pops_TPM.txt.gz  │ │ │ ├── GSM4487938_Mouse_Nasal_IFNa_NH11_dge.txt.gz  │ │ │ ├── GSM4487939_Mouse_Nasal_IFNa_NH12_dge.txt.gz  │ │ │ ├── GSM4487940_Mouse_Nasal_IFNa_NH21_dge.txt.gz  │ │ │ ├── GSM4487941_Mouse_Nasal_IFNa_NH22_dge.txt.gz  │ │ │ ├── GSM4487942_Mouse_Nasal_IFNa_NS11_dge.txt.gz  │ │ │ ├── GSM4487943_Mouse_Nasal_IFNa_NS12_dge.txt.gz  │ │ │ ├── GSM4487944_Mouse_Nasal_IFNa_NS21_dge.txt.gz  │ │ │ ├── GSM4487945_Mouse_Nasal_IFNa_NS22_dge.txt.gz  │ │ │ ├── GSM4488282_Mouse_basal_stim_Untreated_0_REP10_TPM.txt.gz  │ │ │ ├── GSM4488283_Mouse_basal_stim_Untreated_0_REP11_TPM.txt.gz  │ │ │ └── GSM4488284_Mouse_basal_stim_Untreated_0_REP12_TPM.txt.gz  │ │ └── GSE148829_RAW.tar  │ ├── GSE75073  │ │ ├── GSE75073_RAW.tar  │ │ ├── GSE75073_Signals_450k.txt.gz  │ │ └── GSE75073_Signals_Epic.txt.gz  │ ├── GSE76073  │ │ ├── GSE76073_Normalized_counts.txt.gz  │ │ └── GSE76073_Raw_counts.txt.gz  │ ├── GSE923_series_matrix.txt.gz  │ ├── human_hoxb1_protein.fasta  │ ├── human_hoxb1_protein.gb  │ ├── human_rhodopsin_protein.fasta  │ ├── human_ttn_genomic_dna.fasta  │ ├── jp_coldata.txt  │ ├── jp_counts.txt  │ ├── jp_features.txt  │ ├── jp_miame.txt  │ ├── my_transcript.fasta  │ └── unzip  │ ├── Andy.csv  │ ├── David.csv  │ ├── John.csv  │ ├── Mike.csv  │ └── Steve.csv  ├── data_rds  │ ├── biomaRt_homology_example.rds  │ ├── biomaRt_homology_example_aa.rds  │ ├── biomaRt_homology_example_dna.rds  │ └── jp_miame.rds  ├── daurII_students  │ ├── R  │ │ └── do_tidy_pertussis.R  │ ├── README.md  │ ├── code  │ │ └── load_data.R  │ ├── data  │ │ ├── CISID_pertussis_10082018.csv  │ │ ├── course_r_packages.tsv  │ │ ├── covid_rivm  │ │ ├── human_rhodopsin_protein.fasta  │ │ ├── human_ttn_genomic_dna.fasta  │ │ └── lesson2  │ │ └── diet_data.zip  │ ├── daurII_students.Rproj  │ └── demos  │ └── 01_demo_functions.Rmd  ├── daurII_students.Rproj  ├── demos  │ ├── 01_demo_functions.Rmd  │ ├── 021_demo_2_agenda.Rmd  │ ├── 021_demo_2_agenda.html  │ ├── 61_heatmaps.Rmd  │ ├── 61_heatmaps.html  │ ├── 62_rnseq_workflow.Rmd  │ └── 62_rnseq_workflow.html  └── markdown  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.Rmd  ├── Alex_Groot_1730201_Capstone_Exam_ABDS_DAUR2_BIOC.html  ├── Les 6.Rmd  ├── Les4.Rmd  ├── Les4.nb.html  ├── Les5.Rmd  ├── Lesson_5_Exercise_a1.Rmd  ├── answers  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.Rmd  │ ├── daur2_lesson51_demo_multiple_sequence_alignment.html  │ ├── daur2_lesson51_demo_multiple_sequence_alignment_files  │ │ └── figure-html  │ │ ├── unnamed-chunk-23-1.png  │ │ ├── unnamed-chunk-24-1.png  │ │ ├── unnamed-chunk-29-1.png  │ │ └── unnamed-chunk-30-1.png  │ ├── daur2_lesson_6_exercise_rnaseq.Rmd  │ ├── daur2_lesson_6_exercise_rnaseq.html  │ ├── daur2_lesson_6_exercise_rnaseq_student_version.Rmd  │ └── daur2_lesson_6_exercise_rnaseq_student_version.html  ├── les1.Rmd  ├── les1.nb.html  ├── les2.Rmd  ├── les2.nb.html  ├── les3.Rmd  └── resources.Rmd  \n\n\n\n  In order to have the folder structured in the Guerrilla analytics framework method, I retroactively moved the files. This is just used as an example to show what a tidier and better organized structure might look like. The big problem with this is that it will  break many, if not all, of the scripts inside due to changing of file paths.\nThe best way to keep files well-managed is to set up a file structure before starting and sticking to it. Keeping a folder with all the raw data with its names the way they were obtained makes sure confusion is kept to a minimum when the data is looked at. Creating seperate data folders enables to track transformed data while preserving the raw data.\nAnd keeping metadata files in all the folders that explain not just folder structure, but importantly also file naming conventions and potentially used abbreviations for them will mean that even years later someone will be able to look at the project and learn what is inside.\nThis portfolio was made with the Guerrilla analytics framework in mind from the start:\n\n\n\nOld folder structure:\n\n\n/home/alexgroot/dsfb2_quarto\n├── 001_CV.qmd\n├── 002_Looking_ahead.qmd\n├── 003_Guerrilla_analytics_framework.qmd\n├── 004_C_elegans.qmd\n├── 005_open_peer_review.qmd\n├── 006_relational_databases.qmd\n├── 007_R_Package.qmd\n├── 008_Zotero.qmd\n├── 008_Zotero.quarto_ipynb\n├── 009_Parameterized_report.qmd\n├── 010_ML_Getting_started.qmd\n├── 404.qmd\n├── LICENSE\n├── _quarto.yml\n├── _site\n├── about.qmd\n├── css\n│   └── 008_Zotero.css\n├── data_output\n│   ├── data_00100\n│   └── data_0060\n│       ├── dengue.csv\n│       ├── dengue.rds\n│       ├── flu.csv\n│       ├── ...\n├── data_raw\n│   ├── data_00100\n│   │   ├── README\n│   │   ├── breast_cancer\n│   │   │   ├── Index\n│   │   │   ├── breast-cancer.data\n│   │   │   └── breast-cancer.names\n│   │   └── heart_failure.csv\n│   ├── data_0040\n│   │   ├── CE.LIQ.FLOW.062_Tidydata.xlsx\n│   │   └── README\n│   ├── data_0050\n│   │   ├── README\n│   │   ├── TMNT Combined Data OSF.csv\n│   │   ├── TMNT Meta-analysis OSF.csv\n│   │   ├── ...\n│   │   ├── osfstorage-archive\n│   │   │   ├── TMNT Combined Data OSF.csv\n│   │   │   ├── TMNT Meta-analysis OSF.csv\n│   │   │   ├── ...\n│   │   │   └── readme.txt\n│   │   ├── osfstorage-archive.zip\n│   │   └── readme.txt\n│   ├── data_0060\n│   │   ├── README\n│   │   ├── dengue_data.csv\n│   │   └── flu_data.csv\n│   └── data_0090\n│       └── data.csv\n├── dsfb2_quarto.Rproj\n├── images\n│   ├── data_00100\n│   │   ├── baseline_accuracy_model_performance.png\n│   │   └── baseline_precision_model_performance.png\n│   ├── data_0050\n│   │   └── original_plot.png\n│   └── projects\n│       ├── 003_Guerrilla_analytics_framework.png\n│       ├── 006_relational_databases.png\n│       ├── ...\n├── index.qmd\n├── projects.ejs\n├── projects.qmd\n├── references\n│   └── 008_nanopore.bib\n├── scripts\n│   └── 009_Parameterized_report.R\n└── styles.css\n  \n\n\n\n\n\n\n\n\nAll the required main files are in the root folder of the project, as required for the website to render, e.g. the quarto documents, yml, and project files.\n\n\nNext there are extra folders required by the rendered site; images, css, _site, references.\n\n\nThen folders used for the R chunks and scripts; data_raw/output, scripts.\n\n\nFinally Github related files such as .gitignore and LICENSE.\n\n\nStructured systems as such make it easier to both find old files and place new ones.",
    "crumbs": [
      "Home",
      "Guerrilla analytics"
    ]
  },
  {
    "objectID": "010_ML_Getting_started.html",
    "href": "010_ML_Getting_started.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source\n\n\n\n\n\nGetting started with machine learning\nMachine learning can be split between supervised learning and unsupervised learning. Supervised learning requires labelled input and unsupervised learning does not require labelled input data. Supervised learning is best used for classification and regression-based models while unsupervised is best for clustering and association rules.\nIn this project we make use of a Tidymodels regression classification supervised learning model. The breast cancer dataset contains patient data with attributes. With Tidymodels we utilize attributes, such as age, tumor size and degree of malignance to predict recurrence events.\nModel fitting: A simple example of fitting a model would be having the function for a line ‘y = mx + b’ and finding the values for m and b that get the outcome of y, which corresponds with the training data. Basically input data, analyze the input and retrieve the function to recreate the line with different datasets.\nHyperparameter tuning; Fit 90% of your data and test the remaining 10% test fold. Repeat for each separate fold and pick the best results.\n\nLoad R packages\n\n\nLibrary\nlibrary(here)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tibble)\ntidymodels_prefer() # prefer to use functions from tidymodels package\nlibrary(ggplot2)\nlibrary(parsnip)\nlibrary(pROC)\nlibrary(stringr)\n\n\n\n\nLoad data\n\n\nCode\ndata_raw &lt;- as_tibble(read.csv(here::here(\"data_raw/data_00100/breast_cancer/breast-cancer.data\"), header = FALSE))\ndata_lines &lt;- grep(\"\\\\s+[0-9]+\\\\.\\\\s+[A-Za-z-]+:\", \n                   readLines(here::here(\"data_raw/data_00100/breast_cancer/breast-cancer.names\")), value = TRUE)\ndata_names &lt;- sub(\"\\\\s+[0-9]+\\\\.\\\\s+([A-Za-z-]+):.*\", \"\\\\1\", data_lines)[1:10]\ndata_names &lt;- data_names %&gt;% gsub(pattern = \"-\", replacement = \"_\")\n\ndata_tbl &lt;- stats::setNames(data_raw, data_names)\n\n\n\n\nInspect variables\n\n\nCode\nhead(data_tbl) %&gt;%\n  kableExtra::kbl()\n\n\n\n\n\nClass\nage\nmenopause\ntumor_size\ninv_nodes\nnode_caps\ndeg_malig\nbreast\nbreast_quad\nirradiat\n\n\n\n\nno-recurrence-events\n30-39\npremeno\n30-34\n0-2\nno\n3\nleft\nleft_low\nno\n\n\nno-recurrence-events\n40-49\npremeno\n20-24\n0-2\nno\n2\nright\nright_up\nno\n\n\nno-recurrence-events\n40-49\npremeno\n20-24\n0-2\nno\n2\nleft\nleft_low\nno\n\n\nno-recurrence-events\n60-69\nge40\n15-19\n0-2\nno\n2\nright\nleft_up\nno\n\n\nno-recurrence-events\n40-49\npremeno\n0-4\n0-2\nno\n2\nright\nright_low\nno\n\n\nno-recurrence-events\n60-69\nge40\n15-19\n0-2\nno\n2\nleft\nleft_low\nno\n\n\n\n\n\n\n\nCode\n# # TODO # create concat list\nsummary(data_tbl) # summarize dataset\n\n\n    Class               age             menopause          tumor_size       \n Length:286         Length:286         Length:286         Length:286        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  inv_nodes          node_caps           deg_malig        breast         \n Length:286         Length:286         Min.   :1.000   Length:286        \n Class :character   Class :character   1st Qu.:2.000   Class :character  \n Mode  :character   Mode  :character   Median :2.000   Mode  :character  \n                                       Mean   :2.049                     \n                                       3rd Qu.:3.000                     \n                                       Max.   :3.000                     \n breast_quad          irradiat        \n Length:286         Length:286        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nCode\nsum(is.na(data_tbl)) # check for NA values\n\n\n[1] 0\n\n\nCode\n# Change chr to factor\ndata_cols &lt;- colnames(data_tbl[c(\"Class\", \"menopause\", \"node_caps\", \"breast\", \"breast_quad\", \"irradiat\")])\ndata_tbl[data_cols] &lt;- lapply(data_tbl[data_cols], factor)\ndata_tbl\n\n\n# A tibble: 286 × 10\n   Class         age   menopause tumor_size inv_nodes node_caps deg_malig breast\n   &lt;fct&gt;         &lt;chr&gt; &lt;fct&gt;     &lt;chr&gt;      &lt;chr&gt;     &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; \n 1 no-recurrenc… 30-39 premeno   30-34      0-2       no                3 left  \n 2 no-recurrenc… 40-49 premeno   20-24      0-2       no                2 right \n 3 no-recurrenc… 40-49 premeno   20-24      0-2       no                2 left  \n 4 no-recurrenc… 60-69 ge40      15-19      0-2       no                2 right \n 5 no-recurrenc… 40-49 premeno   0-4        0-2       no                2 right \n 6 no-recurrenc… 60-69 ge40      15-19      0-2       no                2 left  \n 7 no-recurrenc… 50-59 premeno   25-29      0-2       no                2 left  \n 8 no-recurrenc… 60-69 ge40      20-24      0-2       no                1 left  \n 9 no-recurrenc… 40-49 premeno   50-54      0-2       no                2 left  \n10 no-recurrenc… 40-49 premeno   20-24      0-2       no                2 right \n# ℹ 276 more rows\n# ℹ 2 more variables: breast_quad &lt;fct&gt;, irradiat &lt;fct&gt;\n\n\nCode\n# change chr to numeric\nrange_to_numeric &lt;- function(column) {\n  column &lt;- str_extract(column, \"\\\\d+-\\\\d+\") # Extract range\n  column &lt;- (as.numeric(str_extract(column, \"^\\\\d+\")) +\n               as.numeric(str_extract(column, \"\\\\d+$\"))) / 2 # Calc middle\n}\n\ndata_input &lt;- data_tbl %&gt;%\n  mutate(across(c(age, tumor_size, inv_nodes), range_to_numeric))\n\n# rename Class to recurrence\nnames(data_input)[1] &lt;- \"recurrence\"\n\ndata_input %&gt;%\n  ggplot(aes(x = recurrence)) +\n  geom_bar() +\n  labs(title = \"Recurrence events\")\n\n\n\n\n\n\n\n\n\nCode\ndata_input %&gt;%\n  ggplot(aes(x = age)) +\n  geom_histogram(colour = \"black\",\n                 fill = \"grey\",\n                 binwidth = 9) +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\n\n\n\n\nSplit into training and testing\n\n\nCode\nset.seed(20241115) # current date\nprop_split &lt;- 0.75\nhdata_split &lt;- initial_split(data_input, \n                             prop = prop_split)\nhdata_train &lt;- training(hdata_split) # extract split training data\nhdata_test &lt;- testing(hdata_split) # extract split testing data\n\n\nWe set a seed for the sake of reproducibility. For ease of use todays date is used. We then set the initial split of the data with a proportion. The dataset contains 286 patients across varying ages. We wouldn’t want to set the split too high or too low and skew the data, therefore we opt for a proportion of 0.75 to start.\n\n\nCreate cross validation folds\nWe perform the vfold cross validation:\n\n\nCode\nhdata_folds &lt;- vfold_cv(hdata_train, v = 10)\n\n\nThe training dataset is selected and is split into 10 groups of equal size or “folds”.\n\n\nBuild a recipe\nWe want to model the outcome, recurrence, by modeling all potential variables in the dataset so we use a “.”, you could also age + deg_malig + recurrence, for example.\nA dummy variable is a binary variable used in modeling algorithms to represent the presence or absence of a categorical variable, particularly when calculations require numerical data. - https://www.sciencedirect.com/topics/computer-science/dummy-variable\nIn order to get the best results we normalize the data. Some observations have way higher values in the thousands, whilst others have values that are 0 or 1.\n\n\nCode\n# y ~ x; recurrence ~ allvariables\nhdata_recipe &lt;- recipe(recurrence ~ ., data = hdata_train) %&gt;%\n  # Dummy encoding for nominal variables\n  step_dummy(menopause, node_caps, breast, breast_quad, irradiat) %&gt;%\n  # Normalize numeric variables\n  step_normalize(age, tumor_size, inv_nodes, deg_malig)\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(hdata_recipe)\n\n\n\n\nCode\n# y ~ x; recurrence ~ allvariables\nhdata_recipe &lt;- recipe(recurrence ~ ., data = hdata_train) %&gt;%\n  step_dummy(sex) %&gt;% step_normalize(age, tumor_size, inv_nodes, deg_malig)\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(hdata_recipe)\n\n\nLASSO regression is used to help select the explanatory variables to include.\nModel evaluation: Accuracy: Proportion of data that are predicted correctly. Because it looks at proportion in the way it does it is not as useful for rare cases where a small set of a population is analyzed, e.g. &lt;1% of people have a certain condition, you could say no one has that certain condition and you’d be 99% correct but still doing poorly. ROC AUC: Area under curve\n\n\nCode\ntune_spec_lasso &lt;-\n  logistic_reg(penalty = tune(), # penalty is the selected lambda for the LASSO regression, tune() makes it\n               mixture = 1) %&gt;% # mixture = 1 = LASSO regression, 0 = ridge regression model\n  set_engine(\"glmnet\")\n\n\nModels can be used for classification or prediction The default for logistic regression is classification. The penalty argument specifies the value for lambda in the LASSO, we use the tune() function instead of picking a specific value. Setting the mixture to 1 sets the function to LASSO regression while 0 sets it to ridge regression. set_engine sets which r package performs the LASSO regression, the package used in this case is glmnet.  All other available engines are:\n\n\nCode\nknitr::kable(show_engines(\"linear_reg\"))\n\n\n\n\n\nengine\nmode\n\n\n\n\nlm\nregression\n\n\nglm\nregression\n\n\nglmnet\nregression\n\n\nstan\nregression\n\n\nspark\nregression\n\n\nkeras\nregression\n\n\nbrulee\nregression\n\n\n\n\n\nNext we need to tune the model, fit and choose the best one using tune_grid().  We need to specify 3 things:  What model are we choosing  What data are we fitting it to  What values of lambda do we want to try\n\n\nCode\nlasso_grid &lt;- tune_grid(\n  add_model(wf, tune_spec_lasso),\n  resamples = hdata_folds,\n  grid = grid_regular(penalty(), levels = 30) # 30 different equally spaced lambda\n)\n\nhighest_roc_auc_lasso &lt;- lasso_grid %&gt;% select_best(metric = \"roc_auc\")\nhighest_roc_auc_lasso\n\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0418 Preprocessor1_Model26\n\n\nWe add wf, our workflow containing the recipe, the variables we’re including and pre-processing steps and the model tune_spec_lasso. We want to fit our model to our cross-validation data hdata_folds and the grid contains the list of lambda values we want to try. We use grid_regular to try different values that are regularly spaced, containing penalty(), which has to match up with the penalty() from tune_spec_lasso. And then we add levels = 30 for the amount of different levels of equally spaced values of lambda.\nWe want the output of lasso_grid to then evaluate and select “the best” lambda. The metric we use here is the highest roc under the curve and we save this output as highest_roc_auc_lasso, which we will use to fit our final model.\n\n\nCode\nfinal_lasso &lt;- finalize_workflow(add_model(wf, tune_spec_lasso), highest_roc_auc_lasso)\n\nfinal_lasso\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.0417531893656041\n  mixture = 1\n\nComputational engine: glmnet \n\n\nCode\nlast_fit_metrics &lt;- last_fit(final_lasso, hdata_split) %&gt;%\n  collect_metrics()\n\nlast_fit_metrics\n\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.625 Preprocessor1_Model1\n2 roc_auc     binary         0.685 Preprocessor1_Model1\n3 brier_class binary         0.240 Preprocessor1_Model1\n\n\nFor the last part of fitting the model we utilize finalize_workflow; we add the model with workflow, the fitted logistic regression model and the highest roc auc.\nThen we evaluate the model using last_fit(), adding the final_lasso and hdata_split, our training data, and then we collect_metrics(). We fit the model to our training split and collect the output metrics\nAccuracy and roc_auc did the best with estimate values of 0.625 and 0.685, which is not terribly high.\nIn linear regression you can look at coefficients and conclude that things such as age correlate with the percentage chance to contract a certain disease. Because we use LASSO regression which works differently from regular regression, with its shrinkage towards zero, we can’t interpret the coefficients in the same way we would interpret regular linear regression.\n\n\nCode\nknitr::kable(\nfinal_lasso %&gt;%\n  fit(hdata_train) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip::vi(lambda = highest_roc_auc_lasso$penalty))\n\n\n\n\n\nVariable\nImportance\nSign\n\n\n\n\nnode_caps_yes\n0.5682117\nPOS\n\n\nirradiat_yes\n0.2840091\nPOS\n\n\nmenopause_premeno\n0.2513943\nPOS\n\n\ndeg_malig\n0.2077117\nPOS\n\n\ninv_nodes\n0.1532454\nPOS\n\n\ntumor_size\n0.0410034\nPOS\n\n\nage\n0.0000000\nNEG\n\n\nmenopause_lt40\n0.0000000\nNEG\n\n\nnode_caps_no\n0.0000000\nNEG\n\n\nbreast_right\n0.0000000\nNEG\n\n\nbreast_quad_central\n0.0000000\nNEG\n\n\nbreast_quad_left_low\n0.0000000\nNEG\n\n\nbreast_quad_left_up\n0.0000000\nNEG\n\n\nbreast_quad_right_low\n0.0000000\nNEG\n\n\nbreast_quad_right_up\n0.0000000\nNEG\n\n\n\n\n\nThis tells us what the most important variables are and their positive/negative impact. The table (fig) shows that presence of node caps is the most important factor that positively impacts the recurrence variable.\n\n\nCode\nfinal_lasso_fitted &lt;- fit(final_lasso, data = hdata_train)\npredicted_classes &lt;- predict(final_lasso_fitted, new_data = hdata_test)\npredicted_probs &lt;- predict(final_lasso_fitted, new_data = hdata_test, type = \"prob\")\nnames(predicted_probs) &lt;- c(\"pred_no_recur\", \"pred_recur\")\n\nresults &lt;- hdata_test %&gt;%\n  bind_cols(predicted_classes, predicted_probs)\n\nroc_auc &lt;- results %&gt;%\n  roc_auc(truth = recurrence, pred_no_recur) # Assuming pred_recur is the probability of recurrence = 1\n\n\nWe now test the training data on the test set and use the results to create some visualisation.\n\n\nCode\nresults %&gt;%\n  ggplot(aes(x = pred_recur, fill = as.factor(recurrence))) +\n  geom_histogram(binwidth = 0.03,\n                 position = \"identity\",\n                 alpha = 0.5) +\n  scale_fill_manual(values = c(\"no-recurrence-events\" = \"blue\", \"recurrence-events\" = \"red\"),\n                    labels = c(\"No recurrence\", \"recurrence\")) +\n  labs(title = \"Distribution of Predicted Probabilities\",\n       x = \"Predicted Probability of recurrence\",\n       y = \"Count\",\n       fill = \"Actual Outcome\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGenerally the expected result of a good model would be a good split between the two outcomes and not a lot of overlap. In this case we have quite a bit of overlap, which is not entirely unexpected considering the somewhat lower roc value of 0.685. There is still room for improvement in the hypertuning of parameters.\n\n\nCode\nresults %&gt;%\n  roc_curve(truth = recurrence, pred_recur, event_level = \"second\") %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Corrected ROC Curve\", x = \"1 - Specificity\", y = \"Sensitivity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# roc_obj &lt;- roc(results$recurrence, results$pred_recur, levels = c(0, 1))\n# auc(roc_obj)  # Should return 0.9375\n# plot(roc_obj, col = \"blue\", lwd = 2)\n\n\nIn the ROC curve we can see that it does lean more towards the top end which increases the area under the curve, but it does stick towards the middle.\n\n\nCode\nconf_mat &lt;- results %&gt;%\n  conf_mat(truth = recurrence, estimate = .pred_class)\n\nautoplot(conf_mat, type = \"heatmap\") +\n  scale_fill_gradient(low = \"darkblue\", high = \"orange\") +\n  labs(title = \"Confusion Matrix Heatmap\", x = \"Predicted Class\", y = \"Actual Class\") +\n  theme_minimal()\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nIn the confusion matrix we can clearly see the results of the model on the test dataset. The predictions for the recurrence x recurrence and no-recurrence x no-recurrence is quite good, however 37% of the mistakes of recurrence events being wrongly predicted as no recurrence events, false negatives, is the least desired type of error that could happen in the context of the dataset.\nAccording to the dataset’s baseline model performance:\n\nAccuracyPrecision\n\n\n\n\n\n\n\n\n\nhttps://archive.ics.uci.edu/dataset/14/breast+cancer There are differing ranges of model performance. It might be that this machine learning model might be a bit weaker at predicting the recurrence with the provided data, random forest for example seems to be doing very well comparatively. However there are still optimizations that are able to be performed to potentially increase the Tidymodels’ performance.",
    "crumbs": [
      "Home",
      "Getting started with machine learning"
    ]
  },
  {
    "objectID": "008_Zotero.html",
    "href": "008_Zotero.html",
    "title": "Developing an automated workflow to analyze differential gene expression in cells through long-read Nanopore RNA sequencing",
    "section": "",
    "text": "RNA sequencing (RNA-Seq) technology has, since its inception shortly after next-generation sequencing (NGS), made large advancements in our ability to study the transcriptome (Weber 2015). Long-read RNA-Sequencing technology, such as the Nanopore MinION, enables us to analyze full-length transcripts over short read RNA-Seq data. While very potent, short-read RNA-Seq has its drawbacks due to its short nature, such as when high amounts of repeating sequences are involved and the challenges in overlapping regions (van Dijk et al. 2018). As full-length isoforms are lost due to the fragmenting nature of short-read RNA-Seq, we can instead turn to long-read RNA-Seq. While this technology can suffer from lower throughput and accuracy, it does allow us to directly sequence native RNA sequences. (Ament et al. 2024). With this versatile tool we can analyze the effects of the exposure of human cells to environmental contaminants, such as microplastics (MPs).\n\n\n\nOver the past decade we have gone from detecting and identifying MPs in marine habitats and oceans worldwide (Thompson et al. 2004) to analyzing human samples and finding them in large groups of people everywhere such as in testicles (Zhao et al. 2023), breastmilk (Saraluck et al. 2024), placenta (Ragusa et al. 2021), liver (Barceló, Picó, and Alfarhan 2023) and respiratory systems (Amato-Lourenço et al. 2021). Since then there has been increasingly more research regarding the topic as evidenced by numerous publications exploring the many routes of exposure and their pathways of MP toxicity. And while it is observed that high concentrations of MPs can cause inflammatory lesions, and may be a factor in the increasing incidence of neurodegenerative diseases, immune disorders and cancers (Prata et al. 2020), the interactions and long-term effects require more studies as they are not yet fully understood. One such pathway is the gene expression, Lei et al. (2018) found that the exposing zebrafish Danio Rerio and nematode Caenorhabditis elegans to MPs caused the intestine to have reduced calcium levels and increased expression of gluthathione S-transferase which indicated intestinal damage and oxidative stress.\n\n\n\nIn order to better understand the biological effect of MPs we aim to utilize long-read RNA-Seq analyses, facilitated by the Oxford Nanopore MinION, to explore the impact on human gene expression to analyze potential downstream health issues. By developing an automated data science workflow which makes use of minimap2, bambu and DESeq2 for discovery, quantification and differential gene expression we aim to provide a workflow that serves as a foundation for our research and a potential foundation for more reproducible data analyses in the future.",
    "crumbs": [
      "Home",
      "RNAseq introduction"
    ]
  },
  {
    "objectID": "008_Zotero.html#rna-sequencing",
    "href": "008_Zotero.html#rna-sequencing",
    "title": "Developing an automated workflow to analyze differential gene expression in cells through long-read Nanopore RNA sequencing",
    "section": "",
    "text": "RNA sequencing (RNA-Seq) technology has, since its inception shortly after next-generation sequencing (NGS), made large advancements in our ability to study the transcriptome (Weber 2015). Long-read RNA-Sequencing technology, such as the Nanopore MinION, enables us to analyze full-length transcripts over short read RNA-Seq data. While very potent, short-read RNA-Seq has its drawbacks due to its short nature, such as when high amounts of repeating sequences are involved and the challenges in overlapping regions (van Dijk et al. 2018). As full-length isoforms are lost due to the fragmenting nature of short-read RNA-Seq, we can instead turn to long-read RNA-Seq. While this technology can suffer from lower throughput and accuracy, it does allow us to directly sequence native RNA sequences. (Ament et al. 2024). With this versatile tool we can analyze the effects of the exposure of human cells to environmental contaminants, such as microplastics (MPs).",
    "crumbs": [
      "Home",
      "RNAseq introduction"
    ]
  },
  {
    "objectID": "008_Zotero.html#microplastics",
    "href": "008_Zotero.html#microplastics",
    "title": "Developing an automated workflow to analyze differential gene expression in cells through long-read Nanopore RNA sequencing",
    "section": "",
    "text": "Over the past decade we have gone from detecting and identifying MPs in marine habitats and oceans worldwide (Thompson et al. 2004) to analyzing human samples and finding them in large groups of people everywhere such as in testicles (Zhao et al. 2023), breastmilk (Saraluck et al. 2024), placenta (Ragusa et al. 2021), liver (Barceló, Picó, and Alfarhan 2023) and respiratory systems (Amato-Lourenço et al. 2021). Since then there has been increasingly more research regarding the topic as evidenced by numerous publications exploring the many routes of exposure and their pathways of MP toxicity. And while it is observed that high concentrations of MPs can cause inflammatory lesions, and may be a factor in the increasing incidence of neurodegenerative diseases, immune disorders and cancers (Prata et al. 2020), the interactions and long-term effects require more studies as they are not yet fully understood. One such pathway is the gene expression, Lei et al. (2018) found that the exposing zebrafish Danio Rerio and nematode Caenorhabditis elegans to MPs caused the intestine to have reduced calcium levels and increased expression of gluthathione S-transferase which indicated intestinal damage and oxidative stress.",
    "crumbs": [
      "Home",
      "RNAseq introduction"
    ]
  },
  {
    "objectID": "008_Zotero.html#aim-of-this-project",
    "href": "008_Zotero.html#aim-of-this-project",
    "title": "Developing an automated workflow to analyze differential gene expression in cells through long-read Nanopore RNA sequencing",
    "section": "",
    "text": "In order to better understand the biological effect of MPs we aim to utilize long-read RNA-Seq analyses, facilitated by the Oxford Nanopore MinION, to explore the impact on human gene expression to analyze potential downstream health issues. By developing an automated data science workflow which makes use of minimap2, bambu and DESeq2 for discovery, quantification and differential gene expression we aim to provide a workflow that serves as a foundation for our research and a potential foundation for more reproducible data analyses in the future.",
    "crumbs": [
      "Home",
      "RNAseq introduction"
    ]
  },
  {
    "objectID": "002_Looking_ahead.html",
    "href": "002_Looking_ahead.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nLooking ahead\nTry, for yourself, to answer the following questions:\n\nWhere do I want to be in ~2 years time?\nHow am I doing now with respect to this goal?\nWhat would be the next skill to learn?\nMake a planning on how to start learning this new skill.\nIn 2 years time I will hopefully be working somewhere related to data sciences. As of now I am unsure of exactly where this will be and what skills will be required, therefore I am unable to answer exactly what I will need in order to achieve this goal. After my internship I won’t necessarily be looking for biology-related data science jobs, so I am also looking for skills that would be versatile in multiple fields of data science. One of these would be machine learning which is a skill I’m looking into but of which I’m still unsure as to how it is utilized.\nThe first thing to do would be to look at different companies and job openings to see what skills are required by them and then delve deeper into what these entail and how/what to apply these on.",
    "crumbs": [
      "Home",
      "Looking ahead"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Groot",
    "section": "",
    "text": "Welcome to my personal website. I am a student life sciences majoring in molecular biology at the Hogeschool Utrecht and I have interests in biology, bioinformatics and also general data science.\nI enjoy working with- and analyzing data and I like building stuff, check out my mostly R related Projects page for some things I’ve worked on.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "R Projects\n  \n  Projects related to R programming.\n  \n  \n    \n      \n        \n          Project Nanopore\n        \n        \n          \n        \n      \n    \n      \n        \n          Interactive R Shiny App Report\n        \n        \n          \n        \n      \n    \n      \n        \n          Tidymodels Machine Learning\n        \n        \n          \n        \n      \n    \n      \n        \n          Parameterized Report\n        \n        \n          \n        \n      \n    \n      \n        \n          R Package\n        \n        \n          \n        \n      \n    \n      \n        \n          Open Peer Review\n        \n        \n          \n        \n      \n    \n      \n        \n          C. elegans Analysis\n        \n        \n          \n        \n      \n    \n  \n\n  General Projects\n  \n  \n  \n  \n    \n      \n        \n          RNA Seq Introduction\n        \n        \n          \n        \n      \n    \n      \n        \n          Guerilla Analytics Framework\n        \n        \n          \n        \n      \n    \n      \n        \n          Looking Ahead\n        \n        \n          \n        \n      \n    \n      \n        \n          Curriculum Vitae\n        \n        \n          \n        \n      \n    \n  \n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Projects"
    ]
  },
  {
    "objectID": "005_open_peer_review.html",
    "href": "005_open_peer_review.html",
    "title": "Open peer review",
    "section": "",
    "text": "Library\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(fs)\n\n\nWrite an Rmarkdown report on your findings, including the table above and some information about the article such as general aim, short methods and results. If data is available, try including some\nThis exercise is about identifying reproducibility issues in a scientific publication. The reproducibility is scored by the metrics used here.\nPublications do not always contain a ‘data availability statement’, and those that do do not always contain any data. Statements range from ‘not publicly available’, ‘available on reasonable request’, ‘publicly available’ or ‘publicly available, except not here and there is no link or text supplying said data’.\nWe looked at this paper in this exercise:\nJason C.K. Chan, Krista D. Manley, Dahwi Ahn, Does retrieval potentiate new learning when retrieval stops but new learning continues?, Journal of Memory and Language, Volume 115, 2020, 104150, ISSN 0749-596X https://doi.org/10.1016/j.jml.2020.104150\nThe general aim of the experiment described in the article has to do with a concept called “the forward testing effect”. This is a mechanic that enhances a learner’s ability to learn new materials, which is a result of “interpolated retrieval opportunities” such as brief quizzes between learning sessions. The article examines the persistence of the forward testing effect when the students stop receiving these interpolated retrieval opportunities.\nThis was tested through four different experiments. The general experimental setup was to have undergrad students from Iowa State University participate instructed to study a list of words with the goal of having a test at the end. The students were given learning instructions and were informed that they might randomly be given interpolated tests by the computer. The experiments differed in at which moments the interpolated tests occurred and the experimental results were measured by the student’s final performance.\nThe study suggests that consistently performing these retrieval opportunities provides a benefit to retaining the information as observed by the improved results. When the interpolated testing is stopped however, its advantages are diminished substantially.\nUsing the metrics from the aforementioned initial publication we score this paper as follows:\n\n\n\n\n\n\n\n\nTransparency Criteria\nDefinition\nResponse\n\n\n\n\nStudy Purpose\nA concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.\nYes\n\n\nData Availability Statement\nA statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.\nNo, supplementary only\n\n\nData Location\nWhere the article’s data can be accessed, either raw or processed.\nNo\n\n\nStudy Location\nAuthor has stated in the methods section where the study took place or the data’s country/region of origin.\nNo, participant’s origin only\n\n\nAuthor Review\nThe professionalism of the contact information that the author has provided in the manuscript.\nYes, email\n\n\nEthics Statement\nA statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.\nNo\n\n\nFunding Statement\nA statement within the manuscript indicating whether or not the authors received funding for their research.\nNo\n\n\nCode Availability\nAuthors have shared access to the most updated code that they used in their study, including code used for analysis.\nNo\n\n\n\nWhile the paper states that it has supplementary data available with a doi linking to the paper: https://doi.org/10.1016/j.jml.2020.104150 It does not contain an actual data availability statement as mentioned before, however upon further inspection the results under experiment 1 does contain a broken link https://doi.org//10.17605/OSF.IO/ G2Y93 due to a space and a working url https://osf.io/g2y93 to the OSF version which contains the data used in the paper.\n\n\nCode\nfs::dir_tree(here::here(\"data_raw/data_0050/osfstorage-archive/\"))\n\n\n/home/alexgroot/dsfb2_quarto/data_raw/data_0050/osfstorage-archive/\n├── TMNT Combined Data OSF.csv\n├── TMNT Meta-analysis OSF.csv\n├── TPL-TMNT E1 Data OSF.csv\n├── TPL-TMNT E2 Data OSF.csv\n├── TPL-TMNT E3 Data OSF.csv\n├── TPL-TMNT E4 Data OSF.csv\n└── readme.txt\n\n\nFiles are clearly named and a readme.txt containing metadata file is included.\n\n\nCode\nread.csv(here::here(\"data_raw/data_0050/osfstorage-archive/TPL-TMNT E1 Data OSF.csv\")) %&gt;% head()\n\n\n      Condition Subject Session L1_RclP L2_RclP L3_RclP L4_RclP Int1_4 Int2_4\n1 Always-Tested     300       1    1.00    1.00    1.00    0.93      0      0\n2 Always-Tested     301       2    1.00    1.00    0.93    1.00      0      0\n3 Always-Tested     302       3    0.87    0.87    0.93    0.73      0      0\n4 Always-Tested     303       4    0.73    0.67    0.87    0.73      0      0\n5 Always-Tested     304       1    0.93    0.47    0.80    1.00      0      0\n6 Always-Tested     305       2    0.60    0.73    0.73    0.87      0      0\n  Int3_4 Int_L4 L1_ARC L2_ARC L3_ARC L4_ARC\n1      0      0   1.00   1.00   0.86   1.00\n2      0      0   0.30   0.50   0.84   1.00\n3      0      0   0.05   0.37   1.00   1.00\n4      0      0   0.74   0.45   0.84   0.78\n5      0      0   0.58   0.61   1.00   0.88\n6      0      0  -0.04   0.34   0.47   1.00\n\n\nThe data provided is in tidy format, which makes for easy data manipulation and analysis. The data does not contain any R code, however the analysis in the original paper was performed in Jeffreys’s Amazing Statistics Program (JASP), which is a program written in C++ and QML, but the analyses themselves are written in R using packages from CRAN.\n\n\nCode\n# load data\ndata &lt;- read.csv(here::here(\"data_raw/data_0050/osfstorage-archive/TPL-TMNT E1 Data OSF.csv\"))\n\n# check the data\ndata %&gt;% \n  summary()\n\n\n  Condition            Subject         Session        L1_RclP      \n Length:120         Min.   :101.0   Min.   :1.00   Min.   :0.0000  \n Class :character   1st Qu.:139.5   1st Qu.:1.75   1st Qu.:0.5300  \n Mode  :character   Median :233.5   Median :2.50   Median :0.7300  \n                    Mean   :227.5   Mean   :2.50   Mean   :0.6784  \n                    3rd Qu.:311.2   3rd Qu.:3.25   3rd Qu.:0.8700  \n                    Max.   :353.0   Max.   :4.00   Max.   :1.0000  \n                                                   NA's   :40      \n    L2_RclP          L3_RclP          L4_RclP           Int1_4      \n Min.   :0.0700   Min.   :0.2700   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5825   1st Qu.:0.5825   1st Qu.:0.3300   1st Qu.:0.0000  \n Median :0.7650   Median :0.8000   Median :0.5300   Median :0.0000  \n Mean   :0.7065   Mean   :0.7342   Mean   :0.5281   Mean   :0.1833  \n 3rd Qu.:0.8850   3rd Qu.:0.9300   3rd Qu.:0.7300   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :4.0000  \n NA's   :80       NA's   :80                                        \n     Int2_4          Int3_4           Int_L4        L1_ARC       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0   Min.   :-1.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.: 0.0650  \n Median :0.000   Median :0.0000   Median :1.0   Median : 0.3600  \n Mean   :0.425   Mean   :0.4917   Mean   :1.1   Mean   : 0.3470  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0   3rd Qu.: 0.7525  \n Max.   :8.000   Max.   :4.0000   Max.   :9.0   Max.   : 1.0000  \n                                                NA's   :40       \n     L2_ARC           L3_ARC            L4_ARC       \n Min.   :-0.670   Min.   :-1.0000   Min.   :-3.0000  \n 1st Qu.: 0.330   1st Qu.: 0.4550   1st Qu.: 0.0000  \n Median : 0.650   Median : 0.8400   Median : 0.4550  \n Mean   : 0.525   Mean   : 0.6368   Mean   : 0.3427  \n 3rd Qu.: 0.865   3rd Qu.: 1.0000   3rd Qu.: 1.0000  \n Max.   : 1.000   Max.   : 1.0000   Max.   : 1.0000  \n NA's   :80       NA's   :80                         \n\n\nCode\ndata %&gt;%\n  head()\n\n\n      Condition Subject Session L1_RclP L2_RclP L3_RclP L4_RclP Int1_4 Int2_4\n1 Always-Tested     300       1    1.00    1.00    1.00    0.93      0      0\n2 Always-Tested     301       2    1.00    1.00    0.93    1.00      0      0\n3 Always-Tested     302       3    0.87    0.87    0.93    0.73      0      0\n4 Always-Tested     303       4    0.73    0.67    0.87    0.73      0      0\n5 Always-Tested     304       1    0.93    0.47    0.80    1.00      0      0\n6 Always-Tested     305       2    0.60    0.73    0.73    0.87      0      0\n  Int3_4 Int_L4 L1_ARC L2_ARC L3_ARC L4_ARC\n1      0      0   1.00   1.00   0.86   1.00\n2      0      0   0.30   0.50   0.84   1.00\n3      0      0   0.05   0.37   1.00   1.00\n4      0      0   0.74   0.45   0.84   0.78\n5      0      0   0.58   0.61   1.00   0.88\n6      0      0  -0.04   0.34   0.47   1.00\n\n\nCode\n# we use the data used in the list 4 correct recall\ndata_e1 &lt;- \n  data %&gt;% dplyr::select(`Condition`, `L4_RclP`)\n\n# Assuming your condition column is a factor, if not, convert it to factor\ndata_e1$Condition &lt;- factor(data_e1$Condition)\n\n# Summarize the data to get the average values and standard deviations for each condition\ndata_summary &lt;- data_e1 %&gt;%\n  group_by(Condition) %&gt;%\n  summarize(average = mean(L4_RclP),\n            sd = sd(L4_RclP))\n\n# create the plot\ndata_e1_plot &lt;- ggplot(data = data_summary,\n                       aes(x = average, y = Condition)) +\n                  geom_bar(stat = \"identity\", width = 0.5, fill = \"white\", colour = \"black\") + \n  scale_x_continuous(breaks = seq(0,0.8, by = 0.1)) + # set scale similar to original plot in paper\n                  geom_errorbar(aes(xmin = average - sd, xmax = average + sd), # calculate errorbars using sd\n                                width = 0.2,\n                                color = \"black\") +  # adjust color of error bars\n                  labs(title = \"Memory performance during List 4 recall in Experiment 1\",\n       x = \"Proportion of List 4 Correct Recall\",\n       y = NULL) + # Y-axis are self-explanatory\n  theme_minimal() +\n  theme(aspect.ratio = 1/4)\ndata_e1_plot\n\n\n\n\n\n\n\n\n\nTo compare our replicated plot to the original from the paper:\n\n\n\nCode\none.way &lt;- aov(L4_RclP ~ Condition, data = data_e1)\nsummary(one.way)\n\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nCondition     2  2.634  1.3169   22.51 5.36e-09 ***\nResiduals   117  6.845  0.0585                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nReturns F(2, 117) = 22.51, p &lt; 0.001, whereas the original paper’s result is F(2, 117) = 22.57, p &lt; 0.001.\n\n\nCode\ndata_e1$Condition %&gt;% unique()\n\n\n[1] Always-Tested Early-Tested  Not-Tested   \nLevels: Always-Tested Early-Tested Not-Tested\n\n\nCode\nt.test(\n  L4_RclP ~ Condition, \n  data = data_e1, \n  subset = Condition == \"Always-Tested\" | Condition == \"Not-Tested\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  L4_RclP by Condition\nt = 6.9752, df = 77.554, p-value = 8.992e-10\nalternative hypothesis: true difference in means between group Always-Tested and group Not-Tested is not equal to 0\n95 percent confidence interval:\n 0.2583119 0.4646881\nsample estimates:\nmean in group Always-Tested    mean in group Not-Tested \n                     0.7180                      0.3565 \n\n\nMean Always-Tested = 0.72 and Not-Tested 0.36 with t(78) = 6.98 , p &lt; 0.001\n\n\nCode\nt.test(\n  L4_RclP ~ Condition,\n  data = data_e1,\n  subset = Condition == \"Early-Tested\" | Condition == \"Not-Tested\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  L4_RclP by Condition\nt = 2.7321, df = 77.484, p-value = 0.007792\nalternative hypothesis: true difference in means between group Early-Tested and group Not-Tested is not equal to 0\n95 percent confidence interval:\n 0.04156587 0.26493413\nsample estimates:\nmean in group Early-Tested   mean in group Not-Tested \n                   0.50975                    0.35650 \n\n\nMean Early-Tested = 0.51, t(78) = 2.73, p = 0.008\n\n\nCode\nt.test(\n  L4_RclP ~ Condition,\n  data = data_e1,\n  subset = Condition == \"Early-Tested\" | Condition == \"Always-Tested\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  L4_RclP by Condition\nt = 3.8389, df = 76.136, p-value = 0.0002537\nalternative hypothesis: true difference in means between group Always-Tested and group Early-Tested is not equal to 0\n95 percent confidence interval:\n 0.1002101 0.3162899\nsample estimates:\nmean in group Always-Tested  mean in group Early-Tested \n                    0.71800                     0.50975 \n\n\nt(78) = 3.84, p &lt; 0,001\nIt is possible to replicate the data from the paper in some way however, not all results are identical. Providing the raw data, rather than the source code, leaves the person replicating the results open to the possibility of making different choices regarding the methods of analysis and is also prone to user-errors. One of the pillars of open science is indeed data, but so is code. And even relatively simple analyses can be interpreted differently by different people, while providing code provides a way to both reproduce the analyses but also provide a vector for other people to learn.",
    "crumbs": [
      "Home",
      "Open peer review"
    ]
  },
  {
    "objectID": "007_R_Package.html",
    "href": "007_R_Package.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nR package"
  },
  {
    "objectID": "006_relational_databases.html",
    "href": "006_relational_databases.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source\n\n\n\n\n\nRelational databases\n\n\nLibrary\nlibrary(here)\nlibrary(dslabs)\nlibrary(tidyverse)\n\n\n\n\nCode\ndengue_raw &lt;- read.csv2(here::here(\"data_raw/data_0060/dengue_data.csv\"),\n                        sep = \",\",\n                        dec = \".\",\n                        skip = 11)\nflu_raw &lt;- read.csv2(here::here(\"data_raw/data_0060/flu_data.csv\"),\n                     sep = \",\",\n                     skip = 11)\ngapminder_raw &lt;- gapminder\n\n\nobjective\nThe Gapminder dataset contains data regarding health and income for 184 countries from 1960 to 2016, such as: country, year, infant mortality per 1000, life expectancy in years, fertility in average number of children per woman, country population, GPD according to World Bankdev, continent and geographical region.\nThe Gapminder dataset is compared to data retrieved from Google trends. The two used datasets contain the Google trend data for the flu and dengue, which is correlated with the occurrence of said diseases in and around its observed trend data.\nThis exercise looks to compare the disease data to the general Gapminder data.\nThis data is then stored to a new PostgreSQL database, analysed and visualized.\n\n\nCode\n# inspecting the data made me annoyed with the fact that View is one of the few functions i use regularly and is capitalised.\nview &lt;- utils::View\n\n# View(dengue_raw) # works\n\n# view(dengue_raw) # does not work, on Mac this view opens in a separate xquartz window, opens in a seperate window in Windows.\n\nview &lt;- function(x, title = deparse(substitute(x))) {\n  if (exists(\"View\", envir = as.environment(\"tools:rstudio\"))) {\n    get(\"View\", envir = as.environment(\"tools:rstudio\"))(x, title)\n  } else {\n    utils::View(x, title)\n  }\n}\n\n# view(dengue_raw) # now checks for the rstudio env and uses it, if rstudio env does not exist default to regular View.\n\n\ninspecting data:\n\n\nCode\ndengue_raw %&gt;% head(n = c(2,4))\n\n\n        Date Argentina Bolivia Brazil\n1 2002-12-29        NA   0.101  0.073\n2 2003-01-05        NA   0.143  0.098\n\n\ndengue raw:    date: YYYY-MM-DD    country: value \n\n\nCode\nna.omit(flu_raw) %&gt;% head(n = c(2,4))\n\n\n          Date Argentina Australia Austria\n160 2006-01-15        91       136     920\n161 2006-01-22        83       121     840\n\n\nCode\n# na.omit is used to show what the data looks like\n\n\nflu raw:    date: YYYY-MM-DD    country: value \n\n\nCode\ngapminder %&gt;% head(n = 2)\n\n\n  country year infant_mortality life_expectancy fertility population\n1 Albania 1960            115.4           62.87      6.19    1636054\n2 Algeria 1960            148.2           47.50      7.65   11124892\n          gdp continent          region\n1          NA    Europe Southern Europe\n2 13828152297    Africa Northern Africa\n\n\ngapminder raw:    date: YYYY-//-//    country: country    life_expectancy: value    fertility: value    population: value    gdp: value \nThe best course of action would be to transform the flu and dengue data to fit into the Gapminder data. For this we need to change the date value into more general “year” values. We can choose between multiple strategies such as taking the highest value for a month, taking a median or an average. Next we need to rename the flu and dengue values with their own headers for easier identification when compared to the Gapminder data.\nWe will lose out some details regarding specific dates but as we’re comparing the data to the Gapminder “yearly” data we won’t be able to use these details.\n\n\nCode\n# date\n# country\n# activity\n\n# tidy data.\n# exclude the date to keep the column intact\ndengue_tidy &lt;-\n  dengue_raw %&gt;% pivot_longer(\n  cols = !`Date`,\n  names_to = \"country\",\n  values_to = \"activity\"\n)\n# change colname to lowercase\ncolnames(dengue_tidy)[1] &lt;- \"date\"\n\n# exclude the date to keep the column intact\nflu_tidy &lt;-\n  flu_raw %&gt;% pivot_longer(\n    cols = !`Date`,\n    names_to = \"country\",\n    values_to = \"activity\"\n  )\n\n# change colname to lowercase\ncolnames(flu_tidy)[1] &lt;- \"date\"\n\n# gapminder is loaded as is\ngapminder_tidy &lt;-\n  gapminder_raw\n\n\n\n\nCode\n# add year column for Gapminder dataset\nflu_tidy$year &lt;- \n  as.Date(flu_tidy$date) %&gt;% format(\"%Y\")\n\ndengue_tidy$year &lt;-\n  as.Date(dengue_tidy$date) %&gt;% format(\"%Y\")\n\n# replace missing NA values with 0\nflu_tidy$activity[is.na(flu_tidy$activity)] &lt;- 0\ndengue_tidy$activity[is.na(dengue_tidy$activity)] &lt;- 0\n\n# group dataframe by country and year, add mean activity column for parity with Gapminder dataset\n\nflu_tidy_mean &lt;-\n  flu_tidy %&gt;% \n  group_by(`country`, `year`) %&gt;% \n  mutate(flu_activity_mean = mean(activity))\n\ndengue_tidy_mean &lt;-\n  dengue_tidy %&gt;% \n  group_by(`country`, `year`) %&gt;% \n  mutate(dengue_tidy_mean = mean(activity))\n\n\n\n\nCode\n# Save files as csv and rds\nwrite.csv(flu_tidy_mean, here(\"data_output/data_0060/flu.csv\"))\nsaveRDS(flu_tidy_mean, here(\"data_output/data_0060/flu.rds\"))\n\nwrite.csv(dengue_tidy_mean, here(\"data_output/data_0060/dengue.csv\"))\nsaveRDS(dengue_tidy_mean, here(\"data_output/data_0060/dengue.rds\"))\n\nwrite.csv(gapminder_tidy, here(\"data_output/data_0060/gapminder.csv\"))\nsaveRDS(gapminder_tidy, here(\"data_output/data_0060/gapminder.rds\"))\n\n# export to SQL\n# db_write_table(con, \"gapminder_table\", gapminder_tidy)"
  }
]